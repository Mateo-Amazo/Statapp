{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /opt/mamba/lib/python3.12/site-packages (16.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/mamba/lib/python3.12/site-packages (from pyarrow) (1.26.4)\n",
      "Requirement already satisfied: pyarrow in /opt/mamba/lib/python3.12/site-packages (16.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/mamba/lib/python3.12/site-packages (from pyarrow) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install --quiet transformers\n",
    "!pip install --quiet datasets\n",
    "!pip install --quiet transformers[torch]\n",
    "!pip install --quiet accelerate -U\n",
    "!pip install --quiet matplotlib\n",
    "!pip install --quiet seaborn\n",
    "!pip install --quiet -U scikit-learn\n",
    "!pip install --quiet nltk\n",
    "!pip install --quiet wandb\n",
    "!pip install pyarrow\n",
    "!pip install --upgrade pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import GPT2Tokenizer, AutoTokenizer\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import GPT2LMHeadModel, AutoConfig\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mamazo/StatApp']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import s3fs\n",
    "import json\n",
    "\n",
    "BUCKET_OUT = \"mamazo\"  \n",
    "\n",
    "S3_ENDPOINT_URL = \"https://\" + os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': S3_ENDPOINT_URL})\n",
    "fs.ls(BUCKET_OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with fs.open(LOGIN+\"/StatApp/tokenized_128_top10.json\", 'r') as file:\n",
    "#   tokenized_test = Dataset.from_dict({\"input_ids\": json.load(file)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "#import nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#stop-words\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BUCKET_OUT = \"mamazo\"\n",
    "file='top_500_sur_10000.json'\n",
    "\n",
    "CORPUS_S3 = BUCKET_OUT + \"/\" + \"StatApp/\" + file\n",
    "with fs.open(CORPUS_S3, 'r') as f:\n",
    "        data = Dataset.from_list(json.load(f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_entropy(matrix, tol = 10**(-2)):\n",
    "    eigenvalues, _ = np.linalg.eig(matrix)\n",
    "    eigenvalues = np.real(eigenvalues)\n",
    "    \n",
    "    for i,val in enumerate(eigenvalues) :\n",
    "        if val < tol :\n",
    "            eigenvalues[i] = 0\n",
    "    \n",
    "    eigenvalues = eigenvalues/np.sum(eigenvalues)\n",
    "\n",
    "    entr = 0\n",
    "    for val in eigenvalues:\n",
    "        #print(val)\n",
    "\n",
    "        if val > 0 :\n",
    "            #print(val*np.log(val))\n",
    "            entr += val*np.log(val)\n",
    "    #print(\"entr :\",entr)\n",
    "\n",
    "    eigenvalues = np.power(eigenvalues,-eigenvalues)\n",
    "\n",
    "    return np.exp(-entr),eigenvalues\n",
    "\n",
    "def select_n_highest(array, n):\n",
    "    sorted_indices = np.argsort(-array)\n",
    "    selected_indices = sorted_indices[:n]\n",
    "    selected_indices.sort()\n",
    "    selected_values = array[selected_indices]\n",
    "    return selected_values, selected_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ce code est beaucoup trop sous-optimisé et ne sert pas à grand chose\n",
    "\n",
    "def maximize_entropy(corpus, threshold_entr = np.inf, max_features=5000, seed_value=345, size_selected=10, n_components = None, affiche_index = False):\n",
    "\n",
    "    n = len(corpus[\"text\"])\n",
    "\n",
    "    if size_selected > n:\n",
    "        size_selected = n\n",
    "    if n_components == None:\n",
    "        n_components = round((size_selected)**(0.5))\n",
    "\n",
    "    np.random.seed(seed_value)\n",
    "\n",
    "    #Sélection du premier texte du corpus\n",
    "    selected_document = np.random.choice(corpus[\"text\"])\n",
    "    index = corpus[\"text\"].index(selected_document)\n",
    "\n",
    "    #Sélection du deuxième texte du corpus\n",
    "\n",
    "    if index == n-1:\n",
    "        index2 = index-1\n",
    "    else :\n",
    "        index2 = index+1\n",
    "    selected_indexes = [index,index2]\n",
    "    \n",
    "    #Création du sous-corpus dans lequel on va ajouter les documents que l'on va sélectionner\n",
    "    selected_text = corpus['text'][index]\n",
    "    selected_text2 = corpus['text'][index2]\n",
    "    sub_corpus = DatasetDict({\"text\": [selected_text,selected_text2]})\n",
    "\n",
    "    #Création du vectorizer\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    vectorizer = TfidfVectorizer(stop_words=stop_words, max_features=max_features)\n",
    "\n",
    "    entr_tot = 0\n",
    "    for k in range(size_selected-2):\n",
    "        \n",
    "        #Création d'un sous-corpus temporaire\n",
    "        sub_corpus2 = sub_corpus\n",
    "\n",
    "        #On ne sélectionne des textes que si on a pas déjà \"suffisamment d'entropie\".\n",
    "        if entr_tot < threshold_entr: \n",
    "            \n",
    "            max_entr_tot = 0\n",
    "            index = 0\n",
    "            for j in range(0,n-1):\n",
    "                \n",
    "                #On trouve un texte que l'on n'a pas déjà utilisé\n",
    "                if j not in selected_indexes:\n",
    "\n",
    "                    #Ajout du texte au corpus temporaire\n",
    "                    selected_text = corpus['text'][j]\n",
    "                    sub_corpus2[\"text\"].append(selected_text)\n",
    "\n",
    "                    #Calcul de la matrice de similarité et de son entropie spectrale\n",
    "                    tfidf_matrix = vectorizer.fit_transform(sub_corpus2)\n",
    "                    lsa_model = TruncatedSVD(n_components=n_components, n_iter=10, random_state=seed_value)\n",
    "                    lsa_matrix = lsa_model.fit_transform(tfidf_matrix)\n",
    "                    similarity_matrix = cosine_similarity(lsa_matrix, lsa_matrix)\n",
    "                    new_entr,aux = spectral_entropy(similarity_matrix)\n",
    "                    \n",
    "                    if new_entr > max_entr_tot:\n",
    "                        index = j\n",
    "            \n",
    "            #On ajoute le texte maximisant l'entropie pour un nombre k+2 de textes du sous-corpus\n",
    "            selected_text = corpus['text'][index]\n",
    "            sub_corpus[\"text\"].append(selected_text)\n",
    "            selected_indexes +=[index]\n",
    "            entr_tot = max_entr_tot\n",
    "\n",
    "    return sub_corpus, entr #On renvoie le sous-corpus ainsi que l'entropie du susnommé\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_least_similar(corpus, max_features=5000, seed_value=345, size_selected=100, tol = 10**(-2), n_components=None, affiche_index=False, affiche_entrop = False, max_df=0.5, min_df=3):\n",
    "\n",
    "    if n_components is None:\n",
    "        n_components = round((size_selected) ** 0.5)\n",
    "\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    vectorizer = TfidfVectorizer(stop_words=stop_words, max_features=max_features, max_df=max_df, min_df=min_df)\n",
    "\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    lsa_model = TruncatedSVD(n_components=n_components, n_iter=10, random_state=seed_value)\n",
    "    lsa_matrix = lsa_model.fit_transform(tfidf_matrix)\n",
    "    similarity_matrix = cosine_similarity(lsa_matrix, lsa_matrix)\n",
    "    \n",
    "    aux,vendi_val = spectral_entropy(similarity_matrix)\n",
    "\n",
    "    selected_vendi,selected_indices = select_n_highest(vendi_val,size_selected)\n",
    "\n",
    "    if affiche_index == True:\n",
    "        print(selected_indices)\n",
    "    \n",
    "    if affiche_entrop == True:\n",
    "        prod = 1\n",
    "        for val in selected_vendi:\n",
    "            prod = prod*val\n",
    "        print(prod)\n",
    "\n",
    "    return data.select(selected_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = Dataset.from_dict(data[:10])\n",
    "\n",
    "#print(select_least_similar(X[\"text\"], size_selected= 9)[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_least_similar2(corpus, num_batch, num_per_batch, max_features=5000, seed_value=345, tol=10**(-2), n_components=None, affiche_index=False, affiche_entrop=False):\n",
    "    \n",
    "    n = len(corpus)\n",
    "    if n_components is None:\n",
    "        n_components = round((num_batch * num_per_batch) ** 0.5)\n",
    "\n",
    "    selected_indices = np.array([], dtype=int)\n",
    "    selected_vendi = np.array([])\n",
    "\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    vectorizer = TfidfVectorizer(stop_words=stop_words, max_features=max_features)\n",
    "\n",
    "    batch_size = int(n // num_batch)\n",
    "    \n",
    "    overall_index = 0\n",
    "    \n",
    "    for k in tqdm(range(num_batch)):\n",
    "        \n",
    "        #print(\"batch n°\",k+1)\n",
    "        sub_corpus = corpus[k * batch_size:(k + 1) * batch_size]\n",
    "        \n",
    "        tfidf_matrix = vectorizer.fit_transform(sub_corpus)\n",
    "        lsa_model = TruncatedSVD(n_components=n_components, n_iter=10, random_state=seed_value)\n",
    "        lsa_matrix = lsa_model.fit_transform(tfidf_matrix)\n",
    "        similarity_matrix = cosine_similarity(lsa_matrix, lsa_matrix)\n",
    "\n",
    "        aux,vendi_val = spectral_entropy(similarity_matrix,tol)\n",
    "\n",
    "        selected_vendi_temp, selected_indices_temp = select_n_highest(vendi_val, num_per_batch)\n",
    "        selected_indices_temp += overall_index\n",
    "        \n",
    "        overall_index += batch_size\n",
    "        \n",
    "        selected_indices = np.concatenate((selected_indices, selected_indices_temp))\n",
    "        selected_vendi = np.concatenate((selected_vendi, selected_vendi_temp))\n",
    "    \n",
    "    selected_indices = selected_indices.astype(int)\n",
    "\n",
    "    if affiche_index:\n",
    "        print(selected_indices)\n",
    "    \n",
    "    if affiche_entrop:\n",
    "\n",
    "        if not isinstance(corpus, np.ndarray):\n",
    "            corpus = np.array(corpus)\n",
    "        \n",
    "        if corpus.ndim > 1:\n",
    "            corpus = corpus.ravel()\n",
    "        \n",
    "        sub_corpus = corpus[selected_indices]\n",
    "        tfidf_matrix = vectorizer.fit_transform(sub_corpus)\n",
    "        lsa_model = TruncatedSVD(n_components=n_components, n_iter=10, random_state=seed_value)\n",
    "        lsa_matrix = lsa_model.fit_transform(tfidf_matrix)\n",
    "        similarity_matrix = cosine_similarity(lsa_matrix, lsa_matrix)\n",
    "\n",
    "        aux,vendi_val = spectral_entropy(similarity_matrix,tol)\n",
    "        print(\"vendi score: \",aux)\n",
    "\n",
    "\n",
    "    return data.select(selected_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:28<00:00,  1.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'url', 'title', 'text'],\n",
       "    num_rows: 5000\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = Dataset.from_dict(data[:10000])\n",
    "\n",
    "select_least_similar2(X[\"text\"], num_batch = 100, num_per_batch = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_least_similar3(corpus, size_selected, num_subcorpus = 10, max_features = 5000, seed_value = 345, tol = 10**(-2), n_components = None, affiche_entrop = False):\n",
    "    \n",
    "    n = len(corpus)\n",
    "\n",
    "    if n_components is None:\n",
    "        n_components = round((size_selected) ** 0.5)\n",
    "    max_vendi = 0\n",
    "\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    vectorizer = TfidfVectorizer(stop_words=stop_words, max_features=max_features)\n",
    "    selected_indices = []\n",
    "\n",
    "    for i in tqdm(range(num_subcorpus)) :\n",
    "        \n",
    "        np.random.seed(seed_value + i)\n",
    "        random_indices = np.random.choice(n, size_selected, replace=False)\n",
    "        sub_corpus = [corpus[idx] for idx in random_indices]\n",
    "\n",
    "        tfidf_matrix = vectorizer.fit_transform(sub_corpus)\n",
    "        lsa_model = TruncatedSVD(n_components=n_components, n_iter=10, random_state=seed_value)\n",
    "        lsa_matrix = lsa_model.fit_transform(tfidf_matrix)\n",
    "        similarity_matrix = cosine_similarity(lsa_matrix, lsa_matrix)\n",
    "\n",
    "        vendi, aux = spectral_entropy(similarity_matrix, tol)\n",
    "        #print(\"vendi :\",vendi)\n",
    "\n",
    "        if vendi > max_vendi :\n",
    "            selected_indices = random_indices\n",
    "            max_vendi = vendi\n",
    "    \n",
    "    if affiche_entrop:\n",
    "        print(max_vendi)\n",
    "\n",
    "    return data.select(selected_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [02:50<00:00, 10.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.474200044772722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'url', 'title', 'text'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = Dataset.from_dict(data[:1000])\n",
    "\n",
    "select_least_similar3(X[\"text\"], size_selected = 500, num_subcorpus =  17, tol = 10**(-2), affiche_entrop = True )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
